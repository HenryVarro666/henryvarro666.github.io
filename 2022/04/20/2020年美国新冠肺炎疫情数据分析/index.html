<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.1.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"henryvarro666.github.io","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":true},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"./public/search.xml"};
  </script>

  <meta name="description" content="使用的Linux(Ubuntu 16.04)系统，运用了Hadoop3.1.3环境、Spark2.4.0大数据分析引擎、Python3.8编程语言、Jupyter Notebook工具。 2020年美国新冠肺炎疫情数据分析 - DomCode 2020年美国新冠肺炎疫情数据分析 厦门大学数据库实验室-林子雨老师">
<meta property="og:type" content="article">
<meta property="og:title" content="2020年美国新冠肺炎疫情数据分析">
<meta property="og:url" content="https://henryvarro666.github.io/2022/04/20/2020%E5%B9%B4%E7%BE%8E%E5%9B%BD%E6%96%B0%E5%86%A0%E8%82%BA%E7%82%8E%E7%96%AB%E6%83%85%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/index.html">
<meta property="og:site_name" content="我见青山多妩媚，料青山 见我应如是">
<meta property="og:description" content="使用的Linux(Ubuntu 16.04)系统，运用了Hadoop3.1.3环境、Spark2.4.0大数据分析引擎、Python3.8编程语言、Jupyter Notebook工具。 2020年美国新冠肺炎疫情数据分析 - DomCode 2020年美国新冠肺炎疫情数据分析 厦门大学数据库实验室-林子雨老师">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204210239851.png">
<meta property="og:image" content="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201309025.png">
<meta property="og:image" content="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201311935.png">
<meta property="og:image" content="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201314994.png">
<meta property="og:image" content="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201314978.png">
<meta property="og:image" content="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201317114.png">
<meta property="og:image" content="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201320358.png">
<meta property="og:image" content="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201326815.png">
<meta property="og:image" content="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201329517.png">
<meta property="og:image" content="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201332988.png">
<meta property="og:image" content="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201336483.png">
<meta property="og:image" content="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201349516.png">
<meta property="og:image" content="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201355780.png">
<meta property="og:image" content="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201415897.png">
<meta property="og:image" content="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201513833.png">
<meta property="og:image" content="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201539985.png">
<meta property="og:image" content="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201555859.png">
<meta property="og:image" content="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201600457.png">
<meta property="og:image" content="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201607765.png">
<meta property="og:image" content="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201626747.png">
<meta property="og:image" content="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201609476.png">
<meta property="og:image" content="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201610847.png">
<meta property="og:image" content="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201615116.png">
<meta property="article:published_time" content="2022-04-20T16:52:34.000Z">
<meta property="article:modified_time" content="2022-04-21T06:45:44.692Z">
<meta property="article:author" content="Chao Cao">
<meta property="article:tag" content="DATA605">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204210239851.png">

<link rel="canonical" href="https://henryvarro666.github.io/2022/04/20/2020%E5%B9%B4%E7%BE%8E%E5%9B%BD%E6%96%B0%E5%86%A0%E8%82%BA%E7%82%8E%E7%96%AB%E6%83%85%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>2020年美国新冠肺炎疫情数据分析 | 我见青山多妩媚，料青山 见我应如是</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">我见青山多妩媚，料青山 见我应如是</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Chao の Blog</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://henryvarro666.github.io/2022/04/20/2020%E5%B9%B4%E7%BE%8E%E5%9B%BD%E6%96%B0%E5%86%A0%E8%82%BA%E7%82%8E%E7%96%AB%E6%83%85%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/head2.jpg">
      <meta itemprop="name" content="Chao Cao">
      <meta itemprop="description" content="为天地立心，为生民立命，为往圣继绝学，为万世开太平">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="我见青山多妩媚，料青山 见我应如是">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          2020年美国新冠肺炎疫情数据分析
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-04-20 11:52:34" itemprop="dateCreated datePublished" datetime="2022-04-20T11:52:34-05:00">2022-04-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-04-21 01:45:44" itemprop="dateModified" datetime="2022-04-21T01:45:44-05:00">2022-04-21</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AF%BE%E7%A8%8B%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">课程学习</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>10k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>9 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>使用的Linux(Ubuntu 16.04)系统，运用了Hadoop3.1.3环境、Spark2.4.0大数据分析引擎、Python3.8编程语言、Jupyter Notebook工具。</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43094619/article/details/117928134">2020年美国新冠肺炎疫情数据分析 - DomCode</a></p>
<p><a target="_blank" rel="noopener" href="http://dblab.xmu.edu.cn/blog/2636-2/">2020年美国新冠肺炎疫情数据分析 厦门大学数据库实验室-林子雨老师</a></p>
<span id="more"></span>

<h1 id="Links"><a href="#Links" class="headerlink" title="Links"></a>Links</h1><p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=p6xDCz00TxU">AWS EKS - Create Kubernetes cluster on Amazon EKS | the easy way</a></p>
<p><a target="_blank" rel="noopener" href="http://dblab.xmu.edu.cn/blog/2441-2/">Hadoop3.1.3安装教程_单机&#x2F;伪分布式配置_Hadoop3.1.3&#x2F;Ubuntu18.04(16.04)</a></p>
<p><a target="_blank" rel="noopener" href="http://dblab.xmu.edu.cn/blog/2501-2/">Spark安装和编程实践（Spark2.4.0）</a></p>
<p><a target="_blank" rel="noopener" href="http://dblab.xmu.edu.cn/blog/2575-2/">使用Jupyter Notebook调试PySpark程序</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/Unitech/pm2">PM2</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/aaPanel/BaoTa">BaoTa</a> &#x2F; <a target="_blank" rel="noopener" href="https://www.aapanel.com/index.html">https://www.aapanel.com/index.html</a></p>
<hr>
<ol>
<li><p>Hadoop：</p>
<p>The Apache™ Hadoop® project develops open-source software for reliable, scalable, <strong><u>distributed computing.</u></strong></p>
<p>The Apache Hadoop software library is a framework that <u>allows for the distributed processing of large data sets across clusters of computers</u> using simple programming models. It is designed to scale up from single servers to thousands of machines, each offering local computation and storage. Rather than rely on hardware to deliver high-availability, <u>the library itself is designed to detect and handle failures at the application layer, so delivering a highly-available service</u> on top of a cluster of computers, each of which may be prone to failures. URL is <a target="_blank" rel="noopener" href="https://hadoop.apache.org/">https://hadoop.apache.org/</a></p>
</li>
<li><p>HDFS：</p>
<p>The Hadoop Distributed File System (HDFS) is a <u>distributed file system</u> designed to run on commodity hardware. It has many similarities with existing distributed file systems. However, the differences from other distributed file systems are significant. <u>HDFS is highly fault-tolerant and is designed to be deployed on low-cost hardware</u>. <u>HDFS provides high throughput access to application data and is suitable for applications that have large data sets.</u> HDFS relaxes a few POSIX requirements to <u>enable streaming access to file system data</u>. HDFS was originally built as infrastructure for the Apache Nutch web search engine project. HDFS is now an Apache Hadoop subproject. The project URL is <a target="_blank" rel="noopener" href="https://hadoop.apache.org/hdfs/">https://hadoop.apache.org/hdfs/</a>.</p>
</li>
<li><p>Spark:</p>
<p>Apache Spark™ is a multi-language <u>engine for executing data engineering, data science, and machine learning on single-node machines or clusters.</u> URL is <a target="_blank" rel="noopener" href="https://spark.apache.org/">https://spark.apache.org/</a></p>
</li>
<li><p>Jupyter Notebook</p>
</li>
<li><p>AWS EKS:</p>
<p>Amazon Elastic Kubernetes Service (Amazon EKS) is a managed container service to run and scale Kubernetes applications in the cloud or on-premises. URL is <a target="_blank" rel="noopener" href="https://aws.amazon.com/eks/?nc1=h_ls">https://aws.amazon.com/eks/?nc1=h_ls</a></p>
<ul>
<li>Deploy applications with Amazon EKS in the cloud </li>
<li>Deploy applications with Amazon EKS Anywhere </li>
<li>Deploy applications with your own tools</li>
</ul>
</li>
<li><p>PM2:</p>
<p>PM2 is a production process manager for Node.js applications with a built-in load balancer. It allows you to keep applications alive forever, to reload them without downtime and to facilitate common system admin tasks.</p>
</li>
<li><p><a href="www.bt.cn">BaoTa</a>:<br>宝塔Linux面板 - 简单好用的服务器运维面板 (<del>要翻墙回去访问，多多少少有毛病</del>我是傻逼，有<a target="_blank" rel="noopener" href="https://www.aapanel.com/index.html">英文版</a>)</p>
<p>宝塔好像是可以搭建docker的</p>
<p><u>宝塔里面可以安装PM2</u></p>
<p><img src="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204210239851.png" alt="image-20220421013933689"></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/api/python/">pyspark</a>:</p>
<p>PySpark is an interface for Apache Spark in Python. I<u>t not only allows you to write Spark applications using Python APIs, but also provides the PySpark shell for interactively analyzing your data in a distributed environment. PySpark supports most of Spark’s features such as Spark SQL, DataFrame, Streaming, MLlib (Machine Learning) and Spark Core.</u></p>
</li>
</ol>
<h1 id="Questions"><a href="#Questions" class="headerlink" title="Questions"></a>Questions</h1><ol>
<li><p>**<u>怎么才能让aws的虚拟机长时间在后台运行？</u>**那个academy好像过几个小时自动关机</p>
<p><a target="_blank" rel="noopener" href="https://awsacademy.instructure.com/login/canvas">awsacademy</a></p>
<p>试试看在宝塔里面运行脚本（记不得是重启机器还是重启进程了）</p>
</li>
<li><p>在aws实例里面搞节点kubernetes啥的？</p>
<p><a target="_blank" rel="noopener" href="https://aws.amazon.com/cn/eks/">Amazon Elastic Kubernetes Service (EKS) 启动、运行和扩展 Kubernetes 的最可信方式</a></p>
<p>但好像Hadoop就可以实现了？</p>
</li>
<li><p>实在不行用自己的网站？</p>
<p>下下策矣</p>
</li>
<li><p>PM2和宝塔好像都是用来部署网页的？</p>
<p>管理文件应该也不差</p>
</li>
<li><p>上课讲到pyspark这玩意儿。能否投入使用?</p>
</li>
<li><p>aws咋进入root模式？？</p>
</li>
</ol>
<h1 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h1><blockquote>
<p>本次实验需要对2020年美国新冠肺炎疫情数据作为数据集，以Python为编程语言，使用Spark对数据进行分析，并对分析结果进行可视化。首先需要对数据的采取，在原始数据集.CSV文件组织的，为了方便spark读取生成RDD或者DataFrame，我们需要对us-counties.csv转换为.txt格式文件us-counties.txt，将文件上传至HDFS文件系统中。其次我们需要对采取的数据进行分析，用python完成本实验要统计的8个指标代码在analyst.py文件中，Spark计算结果保存.json文件，方便后续可视化处理。由于使用Python读取HDFS文件系统不太方便，故将HDFS上结果文件转储到本地文件系统中，在可视化中我们需要安装第三方库pyecharts作为可视化工具，具体可视化实现代码在showdata.py文件中展示，编译之后能产生九副可视化图，展示不一样的图形类型对应的意思也不一样。</p>
</blockquote>
<p>运行环境：</p>
<p>（1）Linux： Ubuntu 16.04<br>（2）Hadoop3.1.3 （<a target="_blank" rel="noopener" href="http://dblab.xmu.edu.cn/blog/2441-2/">查看安装教程</a>）<br>（3）Python: 3.6<br>（4）Spark: 2.4.0 （<a target="_blank" rel="noopener" href="http://dblab.xmu.edu.cn/blog/2501-2/">查看安装教程</a>）<br>（5）Jupyter Notebook （<a target="_blank" rel="noopener" href="http://dblab.xmu.edu.cn/blog/2575-2/">查看安装和使用方法教程</a>）</p>
<h1 id="练习"><a href="#练习" class="headerlink" title="练习"></a>练习</h1><h2 id="用wsl创建本地的Ubuntu虚拟机系统"><a href="#用wsl创建本地的Ubuntu虚拟机系统" class="headerlink" title="用wsl创建本地的Ubuntu虚拟机系统"></a>用wsl创建本地的Ubuntu虚拟机系统</h2><p>VM 或者Virtual Box啥的也可以</p>
<h2 id="在Ubuntu上安装Hadoop"><a href="#在Ubuntu上安装Hadoop" class="headerlink" title="在Ubuntu上安装Hadoop"></a>在Ubuntu上安装Hadoop</h2><h4 id="失败操作"><a href="#失败操作" class="headerlink" title="失败操作"></a>失败操作</h4><hr>
<p>*：<u>该版本暂且作废，以参考博客的附录为准</u></p>
<p><a target="_blank" rel="noopener" href="https://phoenixnap.com/kb/install-hadoop-ubuntu"><strong>How to Install Hadoop on Ubuntu 18.04 or 20.04</strong></a></p>
<p>Install OpenJDK on Ubuntu</p>
<p>At the moment, <strong>Apache Hadoop 3.x fully supports Java 8</strong>. The OpenJDK 8 package in Ubuntu contains both the runtime environment and development kit.</p>
<p><code> sudo apt install openjdk-8-jdk -y</code></p>
<p>安装完检查版本</p>
<p><code> java -version</code></p>
<p><code>javac -version</code></p>
<p><img src="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201309025.png" alt="image-20220420120920974"></p>
<p>Set Up a Non-Root User for Hadoop Environment</p>
<p> To ensure the smooth functioning of Hadoop services, the user should have the ability to establish a <a target="_blank" rel="noopener" href="https://phoenixnap.com/kb/setup-passwordless-ssh">passwordless SSH connection</a> with the localhost.</p>
<p>Install OpenSSH on Ubuntu</p>
<p><code>sudo apt install openssh-server openssh-client -y</code></p>
<p><img src="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201311935.png" alt="image-20220420121150883"></p>
<p>Create Hadoop User</p>
<p><code>sudo adduser hdoop</code></p>
<p><img src="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201314994.png" alt="image-20220420121404929"></p>
<p>（随便设置密码123456）</p>
<p><code>su - hdoop</code>切换用户</p>
<p><img src="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201314978.png" alt="image-20220420121458888"></p>
<p>这条指令注意空格：su - hdoop, 不是su-hdoop</p>
<p>Enable Passwordless SSH for Hadoop User</p>
<p><a target="_blank" rel="noopener" href="https://phoenixnap.com/kb/generate-setup-ssh-key-ubuntu">Generate an SSH key pair </a>and define the location is is to be stored in:</p>
<p><code>ssh-keygen -t rsa -P &#39;&#39; -f ~/.ssh/id_rsa</code></p>
<p><img src="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201317114.png" alt="image-20220420121750040"></p>
<p>Use the <strong><code>cat</code></strong> command to store the public key as <strong>authorized_keys</strong> in the <em>ssh</em> directory:</p>
<p><code>cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</code></p>
<p>Set the permissions for your user with the <strong><code>chmod</code></strong> command:</p>
<p><code>chmod 0600 ~/.ssh/authorized_keys</code></p>
<p>The new user is now able to SSH without needing to enter a password every time. Verify everything is set up correctly by using the <strong>hdoop</strong> user to SSH to localhost:</p>
<p><code>ssh localhost</code></p>
<p>After an initial prompt, the Hadoop user is now able to establish an SSH connection to the localhost seamlessly</p>
<p><img src="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201320358.png" alt="image-20220420122042240"></p>
<p>(nmd, 为什么失败了)</p>
<hr>
<p><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/17335728/connect-to-host-localhost-port-22-connection-refused">connect to host localhost port 22: Connection refused</a></p>
<p><img src="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201326815.png" alt="image-20220420122600667"></p>
<p>没有管理员权限，来回切换真的很烦</p>
<p>hdoop生成的密钥只能hdoop使用。连接成功</p>
<p>Download and Install Hadoop on Ubuntu</p>
<p>安装文件官网：</p>
<p><a target="_blank" rel="noopener" href="https://hadoop.apache.org/releases.html">https://hadoop.apache.org/releases.html</a></p>
<p><img src="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201329517.png" alt="image-20220420122909442"></p>
<p>二进制文件，下载到本地然后编译安装</p>
<p>这边就ubuntu的wget + http下载到本地（先下载再移动进来也可）</p>
<p><code>wget https://dlcdn.apache.org/hadoop/common/hadoop-3.2.3/hadoop-3.2.3.tar.gz</code></p>
<p>解压</p>
<p><code>tar xzf hadoop-3.2.3.tar.gz</code></p>
<p>然后就可以看到压缩文件和解压后的文件夹</p>
<p><img src="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201332988.png" alt="image-20220420123221896"></p>
<p>搞个vim后面好编辑</p>
<p><code>sudo apt-get install vim</code></p>
<p><img src="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201336483.png" alt="image-20220420123620420"></p>
<hr>
<h4 id="Hadoop3-1-3安装教程-单机-x2F-伪分布式配置"><a href="#Hadoop3-1-3安装教程-单机-x2F-伪分布式配置" class="headerlink" title="Hadoop3.1.3安装教程_单机&#x2F;伪分布式配置_"></a><a target="_blank" rel="noopener" href="http://dblab.xmu.edu.cn/blog/2441-2/">Hadoop3.1.3安装教程_单机&#x2F;伪分布式配置_</a></h4><p>Hadoop3.1.3&#x2F;Ubuntu18.04(16.04)</p>
<h5 id="创建新用户"><a href="#创建新用户" class="headerlink" title="创建新用户"></a>创建新用户</h5><p><code>sudo useradd -m hadoop -s /bin/bash</code></p>
<p>设置或者更新密码</p>
<p><code>sudo passwd hadoop</code></p>
<p>添加管理员权限</p>
<p><code>sudo adduser hadoop sudo</code></p>
<p>更换用户</p>
<p><code>su - hadoop</code></p>
<p><img src="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201349516.png" alt="image-20220420124954449"></p>
<p>随手更新apt软件列表</p>
<p><code>sudo apt-get update</code></p>
<p>装个vim</p>
<p><code>sudo apt-get install vim</code></p>
<h4 id="安装SSH、配置SSH无密码登陆"><a href="#安装SSH、配置SSH无密码登陆" class="headerlink" title="安装SSH、配置SSH无密码登陆"></a>安装SSH、配置SSH无密码登陆</h4><p>集群、单节点模式都需要用到 SSH 登陆（类似于远程登陆，你可以登录某台 Linux 主机，并且在上面运行命令），<u><strong>Ubuntu 默认已安装了 SSH client，此外还需要安装 SSH server</strong></u></p>
<p><code>sudo apt-get install openssh-server</code></p>
<p>安装后，可以使用如下命令登陆本机：</p>
<p><code>ssh localhost</code></p>
<p>（md上面那个影响了publickey，这个要重新开始设置）</p>
<p><img src="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201355780.png" alt="image-20220420125531725"></p>
<p>创建密钥</p>
<p>新建钥匙</p>
<p><code>ssh-keygen -t rsa</code></p>
<p>加入授权</p>
<p><code>cat ./id_rsa.pub &gt;&gt; ./authorized_keys </code></p>
<p>删除用户</p>
<p><code>userdel -r newuser</code></p>
<p>因为之前用户，所以出现问题干脆删掉所有公钥密钥主机，重新生成就好了</p>
<p><img src="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201415897.png" alt="image-20220420131509708"></p>
<p><del><u>java-1.8.0-openjdk-amd64</u></del></p>
<p>修改文件名为hadoop</p>
<p><code>sudo mv ./hadoop-3.2.3/ ./hadoop</code></p>
<p>修改文件权限</p>
<p><code>sudo chown -R hadoop ./hadoop</code></p>
<p>安装java</p>
<p>（JDK1.8的安装包jdk-8u162-linux-x64.tar.gz放在了百度云盘，<a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1lLjW9cmS1tkBHkrqtpkjWw">可以点击这里到百度云盘下载JDK1.8安装包</a>（提取码：ziyu））</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/lib/jvm/jdk-8u162-linux-x64</span><br><span class="line">export JRE_HOME=$&#123;JAVA_HOME&#125;/jre</span><br><span class="line">export CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/lib</span><br><span class="line">export PATH=$&#123;JAVA_HOME&#125;/bin:$PATH</span><br></pre></td></tr></table></figure>

<p>一直管理员</p>
<p><code>sudo su</code></p>
<hr>
<p><img src="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201513833.png" alt="image-20220420141306758"></p>
<h5 id="windows-和wsl-传送文件"><a href="#windows-和wsl-传送文件" class="headerlink" title="windows 和wsl 传送文件"></a>windows 和wsl 传送文件</h5><p><del>cp -r C:&#x2F;Users&#x2F;15045&#x2F;Downloads&#x2F;jdk-8u162-linux-x64.tar.gz &#x2F;usr&#x2F;lib&#x2F;jvm</del></p>
<p>wsl访问Windows是mnt</p>
<p>&#x2F;mnt&#x2F;c&#x2F;Users&#x2F;15045&#x2F;Downloads</p>
<p><code>sudo cp jdk-8u162-linux-x64.tar.gz  /usr/lib/jvm</code></p>
<p><code>sudo tar -zxvf jdk-8u162-linux-x64.tar.gz</code></p>
<p>保存.bashrc文件并退出vim编辑器。然后，继续执行如下命令让.bashrc文件的配置立即生效：</p>
<p><code>source ~/.bashrc</code></p>
<p><img src="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201539985.png" alt="image-20220420143956918"></p>
<blockquote>
<p>如果在这一步时提示 <strong>Error: JAVA_HOME is not set and could not be found.</strong> 的错误，则说明之前设置 JAVA_HOME 环境变量那边就没设置好，请按教程先设置好 JAVA_HOME 变量，否则后面的过程都是进行不下去的。</p>
<p>如果已经按照前面教程在.bashrc文件中设置了JAVA_HOME，还是出现 <strong>Error: JAVA_HOME is not set and could not be found.</strong> 的错误，那么，请到hadoop的安装目录修改配置文件“&#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;etc&#x2F;hadoop&#x2F;hadoop-env.sh”，在里面找到“export JAVA_HOME&#x3D;${JAVA_HOME}”这行，然后，把它修改成JAVA安装路径的具体地址，比如，“export JAVA_HOME&#x3D;&#x2F;usr&#x2F;lib&#x2F;jvm&#x2F;default-java”，然后，再次启动Hadoop。</p>
</blockquote>
<h4 id="Hadoop伪分布式配置"><a href="#Hadoop伪分布式配置" class="headerlink" title="Hadoop伪分布式配置"></a>Hadoop伪分布式配置</h4><blockquote>
<p>Hadoop 可以在单节点上以伪分布式的方式运行，Hadoop 进程以分离的 Java 进程来运行，节点既作为 NameNode 也作为 DataNode，同时，读取的是 HDFS 中的文件。</p>
<p>Hadoop 的配置文件位于 &#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;etc&#x2F;hadoop&#x2F; 中，伪分布式需要修改2个配置文件 <strong>core-site.xml</strong> 和 <strong>hdfs-site.xml</strong> 。Hadoop的配置文件是 xml 格式，每个配置以声明 property 的 name 和 value 的方式来实现。</p>
</blockquote>
<p><strong>etc&#x2F;hadoop&#x2F;core-site.xml</strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/usr/local/hadoop/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Abase for other temporary directories.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​      <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><strong>hdfs-site.xml</strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/usr/local/hadoop/tmp/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">​       <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/usr/local/hadoop/tmp/dfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line">   	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p><em>Hadoop配置文件说明</em></p>
<p>Hadoop 的运行方式是由配置文件决定的（运行 Hadoop 时会读取配置文件），因此如果需要从伪分布式模式切换回非分布式模式，需要删除 core-site.xml 中的配置项。</p>
<p>此外，伪分布式虽然只需要配置 fs.defaultFS 和 dfs.replication 就可以运行（官方教程如此），不过若没有配置 hadoop.tmp.dir 参数，则默认使用的临时目录为 &#x2F;tmp&#x2F;hadoo-hadoop，而这个目录在重启时有可能被系统清理掉，导致必须重新执行 format 才行。所以我们进行了设置，同时也指定 dfs.namenode.name.dir 和 dfs.datanode.data.dir，否则在接下来的步骤中可能会出错。</p>
</blockquote>
<h5 id="执行NameNode的格式化"><a href="#执行NameNode的格式化" class="headerlink" title="执行NameNode的格式化"></a>执行NameNode的格式化</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hadoop</span><br><span class="line"></span><br><span class="line">./bin/hdfs namenode -format</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201555859.png" alt="image-20220420145549688"></p>
<h5 id="接着开启-NameNode-和-DataNode-守护进程。"><a href="#接着开启-NameNode-和-DataNode-守护进程。" class="headerlink" title="接着开启 NameNode 和 DataNode 守护进程。"></a>接着开启 NameNode 和 DataNode 守护进程。</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hadoop</span><br><span class="line"></span><br><span class="line">./sbin/start-dfs.sh</span><br></pre></td></tr></table></figure>

<p>报错</p>
<p><img src="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201600457.png" alt="image-20220420150044328"></p>
<p><del><a target="_blank" rel="noopener" href="https://blog.csdn.net/jiecxy/article/details/78003272">https://blog.csdn.net/jiecxy/article/details/78003272</a></del></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/sonetto/article/details/107640438">https://blog.csdn.net/sonetto/article/details/107640438</a></p>
<h5 id="不知道为啥要装ssh和openssh-server"><a href="#不知道为啥要装ssh和openssh-server" class="headerlink" title="不知道为啥要装ssh和openssh-server"></a>不知道为啥要装ssh和openssh-server</h5><p><img src="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201607765.png" alt="image-20220420150730578"></p>
<p>成功！</p>
<p>有可能是因为：</p>
<p><img src="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201626747.png" alt="image-20220420152631577"></p>
<p><img src="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201609476.png" alt="image-20220420150922395"></p>
<h5 id="jps查看启动的hadoop进程"><a href="#jps查看启动的hadoop进程" class="headerlink" title="jps查看启动的hadoop进程"></a>jps查看启动的hadoop进程</h5><p><img src="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201610847.png" alt="image-20220420151001781"></p>
<p>如果DataNode没有启动，可以尝试如下办法（注意这会删除 HDFS 中原有的所有数据，如果原有的数据很重要请不要这样做）：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">针对 DataNode 没法启动的解决方法</span></span><br><span class="line">cd /usr/local/hadoop</span><br><span class="line">./sbin/stop-dfs.sh   # 关闭</span><br><span class="line">rm -r ./tmp     # 删除 tmp 文件，注意这会删除 HDFS 中原有的所有数据</span><br><span class="line">./bin/hdfs namenode -format   # 重新格式化 NameNode</span><br><span class="line">./sbin/start-dfs.sh  # 重启</span><br></pre></td></tr></table></figure>

<h2 id="运行Hadoop伪分布式实例"><a href="#运行Hadoop伪分布式实例" class="headerlink" title="运行Hadoop伪分布式实例"></a>运行Hadoop伪分布式实例</h2><p>上面的<u><strong>单机模式，grep 例子读取的是本地数据，伪分布式读取的则是 HDFS 上的数据</strong></u>。要使用 HDFS，首先需要在 HDFS 中创建用户目录：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hdfs dfs -mkdir -p /user/hadoop</span><br></pre></td></tr></table></figure>



<blockquote>
<p>教材《大数据技术原理与应用》的命令是以”.&#x2F;bin&#x2F;hadoop dfs”开头的Shell命令方式，实际上有三种shell命令方式。<br>\1. hadoop fs<br>\2. hadoop dfs<br>\3. hdfs dfs</p>
<p>hadoop fs适用于任何不同的文件系统，比如本地文件系统和HDFS文件系统<br>hadoop dfs只能适用于HDFS文件系统<br>hdfs dfs跟hadoop dfs的命令作用一样，也只能适用于HDFS文件系统</p>
</blockquote>
<p>接着将 .&#x2F;etc&#x2F;hadoop 中的 xml 文件作为输入文件复制到分布式文件系统中，即将 &#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;etc&#x2F;hadoop 复制到分布式文件系统中的 &#x2F;user&#x2F;hadoop&#x2F;input 中。我们使用的是 hadoop 用户，并且已创建相应的用户目录 &#x2F;user&#x2F;hadoop ，因此在命令中就可以使用相对路径如 input，其对应的绝对路径就是 &#x2F;user&#x2F;hadoop&#x2F;input:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./bin/hdfs dfs -mkdir input</span><br><span class="line"></span><br><span class="line">./bin/hdfs dfs -put ./etc/hadoop/*.xml input</span><br></pre></td></tr></table></figure>

<p>复制完成后，可以通过如下命令查看文件列表：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hdfs dfs -ls input</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/HenryVarro666/images/master/images/202204201615116.png" alt="image-20220420151528022"></p>
<p><u>伪分布式运行 MapReduce 作业的方式跟单机模式相同，区别在于伪分布式读取的是HDFS中的文件（可以将单机步骤中创建的本地 input 文件夹，输出结果 output 文件夹都删掉来验证这一点）。</u></p>
<p>（这部分是Hadoop单机配置的部分，没管）</p>
<p>Hadoop <u>默认模式为非分布式模式（本地模式）</u>，无需进行其他配置即可运行。非分布式即单 Java 进程，方便进行调试。</p>
<hr>
<p>Hadoop 运行程序时，输出目录不能存在，否则会提示错误 “org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs:&#x2F;&#x2F;localhost:9000&#x2F;user&#x2F;hadoop&#x2F;output already exists” ，因此若要再次执行，需要执行如下命令删除 output 文件夹:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hdfs dfs -rm -r output    # 删除 output 文件夹</span><br></pre></td></tr></table></figure>



<blockquote>
<p>运行 Hadoop 程序时，为了防止覆盖结果，程序指定的输出目录（如 output）不能存在，否则会提示错误，因此运行前需要先删除输出目录。在实际开发应用程序时，可考虑在程序中加上如下代码，能在每次运行时自动删除输出目录，避免繁琐的命令行操作</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Configuration conf = new Configuration();</span><br><span class="line"></span><br><span class="line">Job job = new Job(conf);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/\* 删除输出目录 \*/*</span><br><span class="line"></span><br><span class="line">Path outputPath = new Path(args[1]);</span><br><span class="line"></span><br><span class="line">outputPath.getFileSystem(conf).delete(outputPath, true);</span><br></pre></td></tr></table></figure>


</blockquote>
<hr>
<h4 id="若要关闭-Hadoop，则运行"><a href="#若要关闭-Hadoop，则运行" class="headerlink" title="若要关闭 Hadoop，则运行"></a>若要关闭 Hadoop，则运行</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./sbin/stop-dfs.sh</span><br></pre></td></tr></table></figure>



<h4 id="若要关闭-Hadoop，则运行-1"><a href="#若要关闭-Hadoop，则运行-1" class="headerlink" title="若要关闭 Hadoop，则运行"></a>若要关闭 Hadoop，则运行</h4><p>**下次启动 hadoop 时，无需进行 NameNode 的初始化 **</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./sbin/start-dfs.sh</span><br></pre></td></tr></table></figure>



<h2 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h2><p>原始数据集是以.csv文件组织的，为了方便spark读取生成RDD或者DataFrame，首先将us-counties.csv转换为.txt格式文件us-counties.tx </p>
<p><u>Kaggle上面应该有足够的数据集</u>  【us-countries.csv】</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/DATA605/" rel="tag"># DATA605</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/04/20/cmder%E9%85%8D%E7%BD%AE%E9%97%AE%E9%A2%98/" rel="prev" title="cmder配置问题">
      <i class="fa fa-chevron-left"></i> cmder配置问题
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/04/20/Github-lfs%E4%BD%BF%E7%94%A8/" rel="next" title="Github lfs使用">
      Github lfs使用 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Links"><span class="nav-number">1.</span> <span class="nav-text">Links</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Questions"><span class="nav-number">2.</span> <span class="nav-text">Questions</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Example"><span class="nav-number">3.</span> <span class="nav-text">Example</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BB%83%E4%B9%A0"><span class="nav-number">4.</span> <span class="nav-text">练习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%94%A8wsl%E5%88%9B%E5%BB%BA%E6%9C%AC%E5%9C%B0%E7%9A%84Ubuntu%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%B3%BB%E7%BB%9F"><span class="nav-number">4.1.</span> <span class="nav-text">用wsl创建本地的Ubuntu虚拟机系统</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9C%A8Ubuntu%E4%B8%8A%E5%AE%89%E8%A3%85Hadoop"><span class="nav-number">4.2.</span> <span class="nav-text">在Ubuntu上安装Hadoop</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%B1%E8%B4%A5%E6%93%8D%E4%BD%9C"><span class="nav-number">4.2.0.1.</span> <span class="nav-text">失败操作</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Hadoop3-1-3%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B-%E5%8D%95%E6%9C%BA-x2F-%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%85%8D%E7%BD%AE"><span class="nav-number">4.2.0.2.</span> <span class="nav-text">Hadoop3.1.3安装教程_单机&#x2F;伪分布式配置_</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E6%96%B0%E7%94%A8%E6%88%B7"><span class="nav-number">4.2.0.2.1.</span> <span class="nav-text">创建新用户</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%89%E8%A3%85SSH%E3%80%81%E9%85%8D%E7%BD%AESSH%E6%97%A0%E5%AF%86%E7%A0%81%E7%99%BB%E9%99%86"><span class="nav-number">4.2.0.3.</span> <span class="nav-text">安装SSH、配置SSH无密码登陆</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#windows-%E5%92%8Cwsl-%E4%BC%A0%E9%80%81%E6%96%87%E4%BB%B6"><span class="nav-number">4.2.0.3.1.</span> <span class="nav-text">windows 和wsl 传送文件</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Hadoop%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%85%8D%E7%BD%AE"><span class="nav-number">4.2.0.4.</span> <span class="nav-text">Hadoop伪分布式配置</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%89%A7%E8%A1%8CNameNode%E7%9A%84%E6%A0%BC%E5%BC%8F%E5%8C%96"><span class="nav-number">4.2.0.4.1.</span> <span class="nav-text">执行NameNode的格式化</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%8E%A5%E7%9D%80%E5%BC%80%E5%90%AF-NameNode-%E5%92%8C-DataNode-%E5%AE%88%E6%8A%A4%E8%BF%9B%E7%A8%8B%E3%80%82"><span class="nav-number">4.2.0.4.2.</span> <span class="nav-text">接着开启 NameNode 和 DataNode 守护进程。</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%8D%E7%9F%A5%E9%81%93%E4%B8%BA%E5%95%A5%E8%A6%81%E8%A3%85ssh%E5%92%8Copenssh-server"><span class="nav-number">4.2.0.4.3.</span> <span class="nav-text">不知道为啥要装ssh和openssh-server</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#jps%E6%9F%A5%E7%9C%8B%E5%90%AF%E5%8A%A8%E7%9A%84hadoop%E8%BF%9B%E7%A8%8B"><span class="nav-number">4.2.0.4.4.</span> <span class="nav-text">jps查看启动的hadoop进程</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%90%E8%A1%8CHadoop%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%9E%E4%BE%8B"><span class="nav-number">4.3.</span> <span class="nav-text">运行Hadoop伪分布式实例</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%8B%A5%E8%A6%81%E5%85%B3%E9%97%AD-Hadoop%EF%BC%8C%E5%88%99%E8%BF%90%E8%A1%8C"><span class="nav-number">4.3.0.1.</span> <span class="nav-text">若要关闭 Hadoop，则运行</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%8B%A5%E8%A6%81%E5%85%B3%E9%97%AD-Hadoop%EF%BC%8C%E5%88%99%E8%BF%90%E8%A1%8C-1"><span class="nav-number">4.3.0.2.</span> <span class="nav-text">若要关闭 Hadoop，则运行</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE"><span class="nav-number">4.4.</span> <span class="nav-text">数据</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Chao Cao"
      src="/uploads/head2.jpg">
  <p class="site-author-name" itemprop="name">Chao Cao</p>
  <div class="site-description" itemprop="description">为天地立心，为生民立命，为往圣继绝学，为万世开太平</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">73</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/HenryVarro666" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;HenryVarro666" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:ccao2679@umd.edu" title="E-Mail → mailto:ccao2679@umd.edu" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://ccao2679.com/" title="http:&#x2F;&#x2F;ccao2679.com" rel="noopener" target="_blank">Chao の Blog</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chao Cao</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">Symbols count total: </span>
    <span title="Symbols count total">187k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">Reading time total &asymp;</span>
    <span title="Reading time total">2:50</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>









<script>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>




  

  

</body>
</html>
